---
description: Guide for defining basic agents and various workflow types (Chain, Parallel, Router, etc.)
globs: ["**/*.py"]
alwaysApply: true
---

# Defining Agents and Workflows

## Basic Agents

Defining an agent is as simple as:

```python
@fast.agent(
  instruction="Given an object, respond only with an estimate of its size."
)
```

### Sending Messages to Agents

We can then send messages to the Agent:

```python
async with fast.run() as agent:
  moon_size = await agent("the moon")
  print(moon_size)
```

### Interactive Chat

Or start an interactive chat with the Agent:

```python
async with fast.run() as agent:
  await agent.interactive()
```

### Complete Agent Application Example

Here is the complete `sizer.py` Agent application, with boilerplate code:

**sizer.py**
```python
import asyncio
from mcp_agent.core.fastagent import FastAgent

# Create the application
fast = FastAgent("Agent Example")

@fast.agent(
  instruction="Given an object, respond only with an estimate of its size."
)
async def main():
  async with fast.run() as agent:
    await agent()

if __name__ == "__main__":
    asyncio.run(main())
```

The Agent can then be run with `uv run sizer.py`.

### Model Specification

Specify a model with the `--model` switch - for example `uv run sizer.py --model sonnet`.

### Instruction from File

You can also pass a `Path` for the instruction - e.g. 

```python
from pathlib import Path

@fast.agent(
  instruction=Path("./sizing_prompt.md")
)
```

## Workflows and MCP Servers

*To generate examples use `fast-agent quickstart workflow`. This example can be run with `uv run workflow/chaining.py`. fast-agent looks for configuration files in the current directory before checking parent directories recursively.*

Agents can be chained to build a workflow, using MCP Servers defined in the `fastagent.config.yaml` file:

**fastagent.config.yaml**
```yaml
# Example of a STDIO sever named "fetch"
mcp:
  servers:
    fetch:
      command: "uvx"
      args: ["mcp-server-fetch"]
```

**social.py**
```python
@fast.agent(
    "url_fetcher",
    "Given a URL, provide a complete and comprehensive summary",
    servers=["fetch"], # Name of an MCP Server defined in fastagent.config.yaml
)
@fast.agent(
    "social_media",
    """
    Write a 280 character social media post for any given text.
    Respond only with the post, never use hashtags.
    """,
)
@fast.chain(
    name="post_writer",
    sequence=["url_fetcher", "social_media"],
)
async def main():
    async with fast.run() as agent:
        # using chain workflow
        await agent.post_writer("http://fast-agent.ai")
```

All Agents and Workflows respond to `.send("message")`. The agent app responds to `.interactive()` to start a chat session.

Saved as `social.py` we can now run this workflow from the command line with:

```bash
uv run workflow/chaining.py --agent post_writer --message "<url>"
```

Add the `--quiet` switch to disable progress and message display and return only the final response - useful for simple automations.

## Workflow Types

**fast-agent** has built-in support for the patterns referenced in Anthropic's [Building Effective Agents](https://www.anthropic.com/research/building-effective-agents) paper.

### Chain

The `chain` workflow offers a declarative approach to calling Agents in sequence:

```python
@fast.chain(
  "post_writer",
  sequence=["url_fetcher","social_media"]
)

# we can them prompt it directly:
async with fast.run() as agent:
  await agent.interactive(agent="post_writer")
```

This starts an interactive session, which produces a short social media post for a given URL. If a _chain_ is prompted it returns to a chat with last Agent in the chain. You can switch agents by typing `@agent-name`.

Chains can be incorporated in other workflows, or contain other workflow elements (including other Chains). You can set an `instruction` to describe it's capabilities to other workflow steps if needed.

Chains are also helpful for capturing content before being dispatched by a `router`, or summarizing content before being used in the downstream workflow.

### Human Input

Agents can request Human Input to assist with a task or get additional context:

```python
@fast.agent(
    instruction="An AI agent that assists with basic tasks. Request Human Input when needed.",
    human_input=True,
)

await agent("print the next number in the sequence")
```

In the example `human_input.py`, the Agent will prompt the User for additional information to complete the task.

### Parallel

The Parallel Workflow sends the same message to multiple Agents simultaneously (`fan-out`), then uses the `fan-in` Agent to process the combined content.

```python
@fast.agent("translate_fr", "Translate the text to French")
@fast.agent("translate_de", "Translate the text to German")
@fast.agent("translate_es", "Translate the text to Spanish")

@fast.parallel(
  name="translate",
  fan_out=["translate_fr","translate_de","translate_es"]
)

@fast.chain(
  "post_writer",
  sequence=["url_fetcher","social_media","translate"]
)
```

If you don't specify a `fan-in` agent, the `parallel` returns the combined Agent results verbatim.

`parallel` is also useful to ensemble ideas from different LLMs.

When using `parallel` in other workflows, specify an `instruction` to describe its operation.

### Evaluator-Optimizer

Evaluator-Optimizers combine 2 agents: one to generate content (the `generator`), and the other to judge that content and provide actionable feedback (the `evaluator`). Messages are sent to the generator first, then the pair run in a loop until either the evaluator is satisfied with the quality, or the maximum number of refinements is reached.

```python
@fast.agent(
    "content_generator",
    "Generate creative content based on user requests",
)
@fast.agent(
    "quality_evaluator",
    "Evaluate content quality and provide specific improvement suggestions",
)
@fast.evaluator_optimizer(
    name="refined_content",
    generator="content_generator",
    evaluator="quality_evaluator",
    min_rating="GOOD",
    max_refinements=3,
)
```

### Router

Routers intelligently direct messages to the most appropriate agent based on the content and context:

```python
@fast.agent("technical_support", "Handle technical questions and troubleshooting")
@fast.agent("general_assistant", "Handle general questions and conversation")
@fast.agent("creative_writer", "Handle creative writing and content generation")

@fast.router(
    name="smart_router",
    agents=["technical_support", "general_assistant", "creative_writer"],
    instruction="Route user requests to the most appropriate specialist agent"
)
```

### Orchestrator

Orchestrators create comprehensive plans and coordinate multiple agents to achieve complex goals:

```python
@fast.agent("researcher", "Conduct thorough research on given topics")
@fast.agent("writer", "Create well-structured written content")
@fast.agent("reviewer", "Review and improve content quality")

@fast.orchestrator(
    name="content_pipeline",
    instruction="Orchestrate the complete content creation process from research to final review",
    agents=["researcher", "writer", "reviewer"],
    plan_type="full"
)
```

## Agent and Workflow Reference

### Calling Agents

Agents can be called in several ways:

```python
# Direct call
response = await agent.agent_name.send("message")

# Interactive mode
await agent.interactive(agent="agent_name")

# Chain execution
await agent.chain_name("input message")
```

## Configuring Agent Request Parameters

### Example

```python
from mcp_agent.core.request_params import RequestParams

@fast.agent(
    name="example_agent",
    instruction="You are a helpful assistant",
    request_params=RequestParams(
        temperature=0.7,
        max_tokens=2048,
        max_iterations=10
    )
)
```

### Available RequestParams Fields

| Field                 | Type            | Default | Description                                                                                                 |
| --------------------- | --------------- | ------- | ----------------------------------------------------------------------------------------------------------- |
| maxTokens             | int             | 2048    | The maximum number of tokens to sample, as requested by the server                                          |
| model                 | string          | None    | The model to use for the LLM generation. Can only be set at Agent creation time                             |
| use_history          | bool            | True    | Agent/LLM maintains conversation history. Does not include applied Prompts                                  |
| max_iterations       | int             | 20      | The maximum number of tool calls allowed in a conversation turn                                             |
| parallel_tool_calls | bool            | True    | Whether to allow simultaneous tool calls                                                                    |
| response_format      | Any             | None    | Response format for structured calls (advanced use). Prefer to use structured with a Pydantic model instead |
| template_vars        | Dict[str,Any] | {}      | Dictionary of template values for dynamic templates. Currently only supported for TensorZero provider       |
| temperature           | float           | None    | Temperature to use for the completion request                                                               |

## Defining Agents

### Basic Agent

```python
@fast.agent(
  name="agent",                          # name of the agent
  instruction="You are a helpful Agent", # base instruction for the agent
  servers=["filesystem"],                # list of MCP Servers for the agent
  #tools={"filesystem": ["tool_1", "tool_2"]  # Filter the tools available to the agent. Defaults to all
  #resources={"filesystem: ["resource_1", "resource_2"]} # Filter the resources available to the agent. Defaults to all
  #prompts={"filesystem": ["prompt_1", "prompt_2"]}  # Filter the prompts available to the agent. Defaults to all.
  model="o3-mini.high",                  # specify a model for the agent
  use_history=True,                      # agent maintains chat history
  request_params=RequestParams(temperature= 0.7), # additional parameters for the LLM (or RequestParams())
  human_input=True,                      # agent can request human input
  elicitation_handler=ElicitationFnT,    # custom elicitation handler (from mcp.client.session)
  api_key="programmatic-api-key",        # specify the API KEY programmatically, it will override which provided in config file or env var
)
```

### Chain

```python
@fast.chain(
  name="chain",                          # name of the chain
  sequence=["agent1", "agent2", ...],    # list of agents in execution order
  instruction="instruction",             # instruction to describe the chain for other workflows
  cumulative=False,                      # whether to accumulate messages through the chain
  continue_with_final=True,              # open chat with agent at end of chain after prompting
)
```

### Parallel

```python
@fast.parallel(
  name="parallel",                       # name of the parallel workflow
  fan_out=["agent1", "agent2"],          # list of agents to run in parallel
  fan_in="aggregator",                   # name of agent that combines results (optional)
  instruction="instruction",             # instruction to describe the parallel for other workflows
  include_request=True,                  # include original request in fan-in message
)
```

### Evaluator-Optimizer

```python
@fast.evaluator_optimizer(
  name="researcher",                     # name of the workflow
  generator="web_searcher",              # name of the content generator agent
  evaluator="quality_assurance",         # name of the evaluator agent
  min_rating="GOOD",                     # minimum acceptable quality (EXCELLENT, GOOD, FAIR, POOR)
  max_refinements=3,                     # maximum number of refinement iterations
)
```

### Router

```python
@fast.router(
  name="route",                          # name of the router
  agents=["agent1", "agent2", "agent3"], # list of agent names router can delegate to
  instruction="routing instruction",     # any extra routing instructions
  servers=["filesystem"],                # list of servers for the routing agent
  #tools={"filesystem": ["tool_1", "tool_2"]  # Filter the tools available to the agent. Defaults to all
  #resources={"filesystem: ["resource_1", "resource_2"]} # Filter the resources available to the agent. Defaults to all
  #prompts={"filesystem": ["prompt_1", "prompt_2"]}  # Filter the prompts available to the agent. Defaults to all
  model="o3-mini.high",                  # specify routing model
  use_history=False,                     # router maintains conversation history
  human_input=False,                     # whether router can request human input
  api_key="programmatic-api-key",        # specify the API KEY programmatically, it will override which provided in config file or env var
)
```

### Orchestrator

```python
@fast.orchestrator(
  name="orchestrator",                   # name of the orchestrator
  instruction="instruction",             # base instruction for the orchestrator
  agents=["agent1", "agent2"],           # list of agent names this orchestrator can use
  model="o3-mini.high",                  # specify orchestrator planning model
  use_history=False,                     # orchestrator doesn't maintain chat history (no effect).
  human_input=False,                     # whether orchestrator can request human input
  plan_type="full",                      # planning approach: "full" or "iterative"
  max_iterations=5,                      # maximum number of full plan attempts, or iterations
  api_key="programmatic-api-key",        # specify the API KEY programmatically, it will override which provided in config file or env var
)
```

### Custom

```python
@fast.custom(
  cls=Custom                             # agent class
  name="custom",                         # name of the custom agent
  instruction="instruction",             # base instruction for the orchestrator
  servers=["filesystem"],                # list of MCP Servers for the agent
  MCP Servers for the agent
  #tools={"filesystem": ["tool_1", "tool_2"]  # Filter the tools available to the agent. Defaults to all
  #resources={"filesystem: ["resource_1", "resource_2"]} # Filter the resources available to the agent. Defaults to all
  #prompts={"filesystem": ["prompt_1", "prompt_2"]}  # Filter the prompts available to the agent. Defaults to all
  model="o3-mini.high",                  # specify a model for the agent
  use_history=True,                      # agent maintains chat history
  request_params=RequestParams(temperature= 0.7), # additional parameters for the LLM (or RequestParams())
  human_input=True,                      # agent can request human input
  elicitation_handler=ElicitationFnT,    # custom elicitation handler (from mcp.client.session)
  api_key="programmatic-api-key",        # specify the API KEY programmatically, it will override which provided in config file or env var
)
```

---

*Source: [https://fast-agent.ai/agents/defining/](https://fast-agent.ai/agents/defining/)*
